#!/usr/bin/env python3
"""
LoRA Training Script Generator
Generates dedicated LoRA training scripts with CPU/CUDA detection
"""

import os
import json
from typing import Dict, Any

class LoRAScriptGenerator:
    def __init__(self):
        pass
    
    def generate_lora_script(self, job_name: str, base_model: str, config: Dict[str, Any], job_id: int) -> str:
        """Generate LoRA training script with CPU/CUDA detection"""
        
        # Extract configuration
        rank = config.get('rank', 8)
        alpha = config.get('alpha', 32)
        dropout = config.get('dropout', 0.05)
        batch_size = config.get('batchSize', 4)
        epochs = config.get('epochs', 3)
        learning_rate = config.get('learningRate', 0.0002)
        use_qlora = config.get('useQLoRA', False)  # Enable QLoRA for large models
        
        # Map Ollama model to HuggingFace ID
        hf_model = self._map_ollama_to_hf(base_model)
        
        # Determine if we need QLoRA (4-bit quantization)
        needs_qlora = use_qlora or self._is_large_model(hf_model)
        
        # Adjust batch size for large models
        if needs_qlora:
            batch_size = min(batch_size, 2)  # Smaller batch for QLoRA
        
        script_content = f'''#!/usr/bin/env python3
"""
LoRA Training Script for {job_name}
Generated by AI Refinement Dashboard - CPU/CUDA Compatible
"""

import os
import json
import torch
import logging
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    TrainingArguments, 
    Trainer,
    BitsAndBytesConfig,
    DataCollatorForLanguageModeling,
    TrainerCallback
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
from datasets import load_dataset, Dataset
import numpy as np
import requests
import sys
from datetime import datetime

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def send_output_to_frontend(job_id, message):
    """Send output to frontend via API"""
    try:
        requests.post(f'http://localhost:5001/api/training-jobs/{{job_id}}/output', 
                    json={{
                        'output': message,
                        'timestamp': datetime.now().isoformat()
                    }}, 
                    timeout=1)
    except:
        pass  # Don't fail training if output update fails

class ProgressCallback(TrainerCallback):
    def __init__(self, job_id):
        self.job_id = job_id
        
    def on_step_end(self, args, state, control, **kwargs):
        if state.global_step % 5 == 0:  # Update every 5 steps
            try:
                progress = 0.2 + (state.global_step / state.max_steps) * 0.6
                requests.post(f'http://localhost:5001/api/training-jobs/{{job_id}}/progress', 
                            json={{
                                'progress': progress,
                                'current_step': state.global_step,
                                'total_steps': state.max_steps,
                                'epoch': state.epoch,
                                'total_epochs': args.num_train_epochs,
                                'step_progress': f"{{state.global_step}}/{{state.max_steps}}"
                            }}, timeout=1)
            except:
                pass  # Don't fail training if progress update fails

def main():
    try:
        job_id = {job_id}
        logger.info("ðŸš€ Starting LoRA training for {job_name}")
        send_output_to_frontend(job_id, "ðŸš€ Starting LoRA training for {job_name}")
        
        # Model and data paths
        base_model = "{hf_model}"
        train_data_path = "training_data/job_{job_id}/train.jsonl"
        val_data_path = "training_data/job_{job_id}/val.jsonl"
        output_dir = f"models/{job_name}_lora"
        use_qlora = {needs_qlora}
        
        # DEBUG: Check if training files exist
        logger.info(f"ðŸ” DEBUG: Checking training files...")
        logger.info(f"ðŸ“ Train file exists: {{os.path.exists(train_data_path)}}")
        logger.info(f"ðŸ“ Val file exists: {{os.path.exists(val_data_path)}}")
        if os.path.exists(train_data_path):
            logger.info(f"ðŸ“ Train file size: {{os.path.getsize(train_data_path)}} bytes")
        if os.path.exists(val_data_path):
            logger.info(f"ðŸ“ Val file size: {{os.path.getsize(val_data_path)}} bytes")
        
        # DEBUG: Check base model mapping
        logger.info(f"ðŸ” DEBUG: Base model: '{{base_model}}'")
        logger.info(f"ðŸ” DEBUG: HF model ID: '{{base_model}}'")
        
        # Load model with appropriate configuration
        logger.info("ðŸ“¥ Loading base model...")
        send_output_to_frontend(job_id, f"ðŸ“¥ Loading base model: {{base_model}}")
        
        # DEBUG: Check GPU capabilities
        logger.info(f"ðŸ” DEBUG: CUDA available: {{torch.cuda.is_available()}}")
        if torch.cuda.is_available():
            logger.info(f"ðŸ” DEBUG: CUDA device count: {{torch.cuda.device_count()}}")
            logger.info(f"ðŸ” DEBUG: Current CUDA device: {{torch.cuda.current_device()}}")
            logger.info(f"ðŸ” DEBUG: CUDA device name: {{torch.cuda.get_device_name()}}")
            logger.info(f"ðŸ” DEBUG: BF16 supported: {{torch.cuda.is_bf16_supported()}}")
        else:
            logger.info(f"ðŸ” DEBUG: Running on CPU - CUDA not available")
        
        # Configure model loading based on CUDA availability and model size
        if torch.cuda.is_available():
            if use_qlora:
                logger.info("ðŸ”§ Loading model with QLoRA (4-bit quantization) for RTX 4050")
                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=torch.float16,  # Use float16 for RTX 4050
                    bnb_4bit_use_double_quant=True,
                )
                
                model = AutoModelForCausalLM.from_pretrained(
                    base_model,
                    quantization_config=bnb_config,
                    device_map="auto",
                    trust_remote_code=True,
                    torch_dtype=torch.float16
                )
                model = prepare_model_for_kbit_training(model)
            else:
                logger.info("ðŸ”§ Loading model with CUDA support (full precision)")
                model = AutoModelForCausalLM.from_pretrained(
                    base_model,
                    device_map="auto",
                    trust_remote_code=True,
                    torch_dtype=torch.float16
                )
        else:
            logger.info("ðŸ”§ Loading model for CPU-only environment")
            model = AutoModelForCausalLM.from_pretrained(
                base_model,
                trust_remote_code=True,
                torch_dtype=torch.float32
            )
        
        tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
        
        # Add padding token if missing (important for Gemma models)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            tokenizer.pad_token_id = tokenizer.eos_token_id
        
        # Prepare model for k-bit training (only if QLoRA)
        if use_qlora and torch.cuda.is_available():
            model = prepare_model_for_kbit_training(model)
        
        # LoRA configuration with better target modules
        lora_config = LoraConfig(
            r={rank},
            lora_alpha={alpha},
            target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj", "lm_head"],
            lora_dropout={dropout},
            bias="none",
            task_type="CAUSAL_LM",
            inference_mode=False,
            init_lora_weights=True
        )
        
        # Apply LoRA
        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()
        
        # Load dataset
        logger.info("ðŸ“Š Loading training data...")
        send_output_to_frontend(job_id, "ðŸ“Š Loading training data...")
        dataset = load_dataset(
            "json",
            data_files={{
                "train": train_data_path,
                "validation": val_data_path
            }},
            streaming=False
        )
        
        # Tokenize function with NLâ†’Code formatting (optimized for Amigo training)
        def tokenize_function(examples):
            # Create instruction format optimized for Natural Language â†’ Code training
            texts = []
            for i in range(len(examples["instruction"])):
                instruction = examples["instruction"][i]
                input_text = examples["input"][i] if "input" in examples and examples["input"][i] else ""
                output = examples["output"][i]
                
                # Use streamlined format for NLâ†’Code (Cursor-style)
                # Skip verbose reasoning for direct code generation
                if input_text:
                    text = f"### Instruction:\\n{{instruction}}\\n\\n### Input:\\n{{input_text}}\\n\\n### Response:\\n{{output}}"
                else:
                    # Direct NLâ†’Code format (no verbose reasoning)
                    text = f"### Instruction:\\n{{instruction}}\\n\\n### Response:\\n{{output}}"
                
                texts.append(text)
            
            # Tokenize with proper settings (longer max_length for code)
            tokenized = tokenizer(
                texts,
                truncation=True,
                padding=True,
                max_length=2048,  # Longer for code generation
                return_tensors="pt"
            )
            
            # Set labels same as input_ids for causal LM
            tokenized["labels"] = tokenized["input_ids"].clone()
            return tokenized
        
        # Tokenize datasets with proper column removal
        train_dataset = dataset["train"].map(tokenize_function, batched=True, remove_columns=dataset["train"].column_names)
        val_dataset = dataset["validation"].map(tokenize_function, batched=True, remove_columns=dataset["validation"].column_names)
        
        # Training arguments - configure based on CUDA availability and model size
        if torch.cuda.is_available():
            if use_qlora:
                logger.info("ðŸ”§ Configuring training for CUDA with QLoRA (optimized for RTX 4050)")
                training_args = TrainingArguments(
                    output_dir=output_dir,
                    per_device_train_batch_size={batch_size},
                    per_device_eval_batch_size={batch_size},
                    gradient_accumulation_steps=4,  # Effective batch = 2 * 4 = 8
                    num_train_epochs={epochs},
                    learning_rate={learning_rate},
                    warmup_ratio=0.1,
                    weight_decay=0.01,
                    fp16=True,  # Mixed precision for QLoRA
                    logging_steps=10,
                    eval_strategy="steps",
                    eval_steps=50,
                    save_steps=100,
                    save_total_limit=3,
                    load_best_model_at_end=True,
                    metric_for_best_model="eval_loss",
                    greater_is_better=False,
                    report_to=None,
                    remove_unused_columns=False,
                    dataloader_pin_memory=False,
                    optim="adamw_torch",
                    lr_scheduler_type="cosine",
                    max_grad_norm=1.0,  # Gradient clipping for stability
                )
            else:
                logger.info("ðŸ”§ Configuring training for CUDA (full precision)")
                training_args = TrainingArguments(
                    output_dir=output_dir,
                    per_device_train_batch_size={batch_size},
                    per_device_eval_batch_size={batch_size},
                    gradient_accumulation_steps=4,
                    num_train_epochs={epochs},
                    learning_rate={learning_rate},
                    warmup_ratio=0.1,
                    weight_decay=0.01,
                    fp16=True,
                    logging_steps=10,
                    eval_strategy="steps",
                    eval_steps=50,
                    save_steps=100,
                    save_total_limit=3,
                    load_best_model_at_end=True,
                    metric_for_best_model="eval_loss",
                    greater_is_better=False,
                    report_to=None,
                    remove_unused_columns=False,
                    dataloader_pin_memory=False,
                    optim="adamw_torch",
                    lr_scheduler_type="cosine",
                )
        else:
            logger.info("ðŸ”§ Configuring training for CPU")
            training_args = TrainingArguments(
                output_dir=output_dir,
                per_device_train_batch_size=1,  # Smaller batch for CPU
                per_device_eval_batch_size=1,
                gradient_accumulation_steps=8,  # More accumulation for CPU
                num_train_epochs=1,  # Fewer epochs for CPU
                learning_rate={learning_rate},
                warmup_ratio=0.1,
                weight_decay=0.01,
                logging_steps=10,
                eval_strategy="steps",
                eval_steps=50,
                save_steps=100,
                save_total_limit=3,
                load_best_model_at_end=True,
                metric_for_best_model="eval_loss",
                greater_is_better=False,
                report_to=None,  # Disable wandb/tensorboard
                remove_unused_columns=False,
                dataloader_pin_memory=False,
                optim="adamw_torch",
                lr_scheduler_type="cosine",
            )
        
        # Create trainer with progress callback
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            tokenizer=tokenizer,
            callbacks=[ProgressCallback(job_id)]
        )
        
        # Start training
        logger.info("ðŸƒ Starting training...")
        send_output_to_frontend(job_id, "ðŸƒ Starting LoRA training...")
        trainer.train()
        
        # Save model
        logger.info("ðŸ’¾ Saving trained model...")
        send_output_to_frontend(job_id, "ðŸ’¾ Saving trained model...")
        trainer.save_model()
        tokenizer.save_pretrained(output_dir)
        
        # Merge and save final model
        logger.info("ðŸ”— Merging LoRA adapters...")
        send_output_to_frontend(job_id, "ðŸ”— Merging LoRA adapters...")
        model = model.merge_and_unload()
        
        # Save merged model
        merged_output_dir = f"models/{job_name}_lora_merged"
        model.save_pretrained(merged_output_dir)
        tokenizer.save_pretrained(merged_output_dir)
        
        logger.info("âœ… LoRA training completed successfully!")
        send_output_to_frontend(job_id, "âœ… LoRA training completed successfully!")
        
    except Exception as e:
        logger.error(f"âŒ Training failed: {{str(e)}}")
        send_output_to_frontend(job_id, f"âŒ Training failed: {{str(e)}}")
        import traceback
        traceback.print_exc()
        raise

if __name__ == "__main__":
    main()
'''
        return script_content
    
    def _map_ollama_to_hf(self, ollama_model: str) -> str:
        """Map Ollama model names to Hugging Face model IDs"""
        mapping = {
            'llama3.1:8b': 'meta-llama/Llama-3.1-8B',
            'llama3.1:70b': 'meta-llama/Llama-3.1-70B',
            'llama3:8b': 'meta-llama/Llama-3-8B',
            'llama3:70b': 'meta-llama/Llama-3-70B',
            'codellama:7b': 'codellama/CodeLlama-7b-hf',
            'codellama:13b': 'codellama/CodeLlama-13b-hf',
            'codellama:34b': 'codellama/CodeLlama-34b-hf',
            'mistral:7b': 'mistralai/Mistral-7B-v0.1',
            'mixtral:8x7b': 'mistralai/Mixtral-8x7B-v0.1',
            'qwen2.5:7b': 'Qwen/Qwen2.5-7B',
            'qwen2.5:14b': 'Qwen/Qwen2.5-14B',
            'qwen2.5:32b': 'Qwen/Qwen2.5-32B',
            'qwen2.5-coder:7b': 'Qwen/Qwen2.5-Coder-7B',
            'qwen2.5-coder:14b': 'Qwen/Qwen2.5-Coder-14B',
            'gemma:2b': 'google/gemma-2b',
            'gemma:7b': 'google/gemma-7b',
            'gemma-2:2b': 'google/gemma-2-2b-it',
            'gemma-2:9b': 'google/gemma-2-9b-it',
            'gemma-2:27b': 'google/gemma-2-27b-it',
            'gemma-3:12b': 'google/gemma-3-12b-it',
            'claude-3.7-sonnet-reasoning-gemma3-12b': 'google/gemma-3-12b-it',  # Amigo base model
            'phi3:3.8b': 'microsoft/Phi-3-medium-4k-instruct',
            'phi3:14b': 'microsoft/Phi-3.5-14b-instruct',
        }
        
        # Clean the model name
        clean_name = ollama_model.lower().strip()
        
        # Check if it's in our mapping
        if clean_name in mapping:
            return mapping[clean_name]
        else:
            # Default fallback - try to use the name as-is
            print(f"âš ï¸ Unknown Ollama model: {ollama_model}, using as Hugging Face ID")
            return clean_name
    
    def _is_large_model(self, hf_model: str) -> bool:
        """Check if model is large (>=9B) and needs QLoRA"""
        large_models = [
            'google/gemma-3-12b-it',
            'google/gemma-2-27b-it',
            'meta-llama/Llama-3.1-70B',
            'Qwen/Qwen2.5-32B',
        ]
        return hf_model in large_models or '12b' in hf_model.lower() or '27b' in hf_model.lower()
